{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7075785,"sourceType":"datasetVersion","datasetId":4075512},{"sourceId":7076509,"sourceType":"datasetVersion","datasetId":4076047},{"sourceId":7076532,"sourceType":"datasetVersion","datasetId":4076067},{"sourceId":7076610,"sourceType":"datasetVersion","datasetId":4076131},{"sourceId":7076805,"sourceType":"datasetVersion","datasetId":4076279},{"sourceId":7085934,"sourceType":"datasetVersion","datasetId":4082638},{"sourceId":7085955,"sourceType":"datasetVersion","datasetId":4082655},{"sourceId":3849,"sourceType":"modelInstanceVersion","modelInstanceId":2750}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"pvtv2.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\nfrom timm.models.vision_transformer import _cfg\nfrom timm.models.registry import register_model\n\nimport math\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        if self.sr_ratio > 1:\n            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n            x_ = self.norm(x_)\n            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        else:\n            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n\n        return x\n\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x, H, W\n\n\nclass PyramidVisionTransformerImpr(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n        super().__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n\n        # patch_embed\n        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n                                              embed_dim=embed_dims[0])\n        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n                                              embed_dim=embed_dims[3])\n\n        # transformer encoder\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        self.block1 = nn.ModuleList([Block(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])\n            for i in range(depths[0])])\n        self.norm1 = norm_layer(embed_dims[0])\n\n        cur += depths[0]\n        self.block2 = nn.ModuleList([Block(\n            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[1])\n            for i in range(depths[1])])\n        self.norm2 = norm_layer(embed_dims[1])\n\n        cur += depths[1]\n        self.block3 = nn.ModuleList([Block(\n            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[2])\n            for i in range(depths[2])])\n        self.norm3 = norm_layer(embed_dims[2])\n\n        cur += depths[2]\n        self.block4 = nn.ModuleList([Block(\n            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[3])\n            for i in range(depths[3])])\n        self.norm4 = norm_layer(embed_dims[3])\n\n        # classification head\n        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = 1\n            #load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n\n    def reset_drop_path(self, drop_path_rate):\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n        cur = 0\n        for i in range(self.depths[0]):\n            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[0]\n        for i in range(self.depths[1]):\n            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[1]\n        for i in range(self.depths[2]):\n            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[2]\n        for i in range(self.depths[3]):\n            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n\n    def freeze_patch_emb(self):\n        self.patch_embed1.requires_grad = False\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    # def _get_pos_embed(self, pos_embed, patch_embed, H, W):\n    #     if H * W == self.patch_embed1.num_patches:\n    #         return pos_embed\n    #     else:\n    #         return F.interpolate(\n    #             pos_embed.reshape(1, patch_embed.H, patch_embed.W, -1).permute(0, 3, 1, 2),\n    #             size=(H, W), mode=\"bilinear\").reshape(1, -1, H * W).permute(0, 2, 1)\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        outs = []\n\n        # stage 1\n        x, H, W = self.patch_embed1(x)\n        for i, blk in enumerate(self.block1):\n            x = blk(x, H, W)\n        x = self.norm1(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 2\n        x, H, W = self.patch_embed2(x)\n        for i, blk in enumerate(self.block2):\n            x = blk(x, H, W)\n        x = self.norm2(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 3\n        x, H, W = self.patch_embed3(x)\n        for i, blk in enumerate(self.block3):\n            x = blk(x, H, W)\n        x = self.norm3(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 4\n        x, H, W = self.patch_embed4(x)\n        for i, blk in enumerate(self.block4):\n            x = blk(x, H, W)\n        x = self.norm4(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        return outs\n\n        # return x.mean(dim=1)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        # x = self.head(x)\n\n        return x\n\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n\n        return x\n\n\ndef _conv_filter(state_dict, patch_size=16):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n        out_dict[k] = v\n\n    return out_dict\n\n\n@register_model\nclass pvt_v2_b0(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b0, self).__init__(\n            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n\n\n@register_model\nclass pvt_v2_b1(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b1, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n@register_model\nclass pvt_v2_b2(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b2, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n@register_model\nclass pvt_v2_b3(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b3, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n@register_model\nclass pvt_v2_b4(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b4, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n\n@register_model\nclass pvt_v2_b5(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b5, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T23:35:29.181729Z","iopub.execute_input":"2023-11-29T23:35:29.182099Z","iopub.status.idle":"2023-11-29T23:35:36.970086Z","shell.execute_reply.started":"2023-11-29T23:35:29.182070Z","shell.execute_reply":"2023-11-29T23:35:36.969069Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"pvt.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n        super(BasicConv2d, self).__init__()\n\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass CFM(nn.Module):\n    def __init__(self, channel):\n        super(CFM, self).__init__()\n        self.relu = nn.ReLU(True)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample5 = BasicConv2d(2 * channel, 2 * channel, 3, padding=1)\n\n        self.conv_concat2 = BasicConv2d(2 * channel, 2 * channel, 3, padding=1)\n        self.conv_concat3 = BasicConv2d(3 * channel, 3 * channel, 3, padding=1)\n        self.conv4 = BasicConv2d(3 * channel, channel, 3, padding=1)\n\n    def forward(self, x1, x2, x3):\n        x1_1 = x1\n        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n               * self.conv_upsample3(self.upsample(x2)) * x3\n\n        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n        x2_2 = self.conv_concat2(x2_2)\n\n        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n        x3_2 = self.conv_concat3(x3_2)\n\n        x1 = self.conv4(x3_2)\n\n        return x1\n\n\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_state, num_node, bias=False):\n        super(GCN, self).__init__()\n        self.conv1 = nn.Conv1d(num_node, num_node, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv1d(num_state, num_state, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        h = self.conv1(x.permute(0, 2, 1)).permute(0, 2, 1)\n        h = h - x\n        h = self.relu(self.conv2(h))\n        return h\n\n\nclass SAM(nn.Module):\n    def __init__(self, num_in=32, plane_mid=16, mids=4, normalize=False):\n        super(SAM, self).__init__()\n\n        self.normalize = normalize\n        self.num_s = int(plane_mid)\n        self.num_n = (mids) * (mids)\n        self.priors = nn.AdaptiveAvgPool2d(output_size=(mids + 2, mids + 2))\n\n        self.conv_state = nn.Conv2d(num_in, self.num_s, kernel_size=1)\n        self.conv_proj = nn.Conv2d(num_in, self.num_s, kernel_size=1)\n        self.gcn = GCN(num_state=self.num_s, num_node=self.num_n)\n        self.conv_extend = nn.Conv2d(self.num_s, num_in, kernel_size=1, bias=False)\n\n    def forward(self, x, edge):\n        edge = F.upsample(edge, (x.size()[-2], x.size()[-1]))\n\n        n, c, h, w = x.size()\n        edge = torch.nn.functional.softmax(edge, dim=1)[:, 1, :, :].unsqueeze(1)\n\n        x_state_reshaped = self.conv_state(x).view(n, self.num_s, -1)\n        x_proj = self.conv_proj(x)\n        x_mask = x_proj * edge\n\n        x_anchor1 = self.priors(x_mask)\n        x_anchor2 = self.priors(x_mask)[:, :, 1:-1, 1:-1].reshape(n, self.num_s, -1)\n        x_anchor = self.priors(x_mask)[:, :, 1:-1, 1:-1].reshape(n, self.num_s, -1)\n\n        x_proj_reshaped = torch.matmul(x_anchor.permute(0, 2, 1), x_proj.reshape(n, self.num_s, -1))\n        x_proj_reshaped = torch.nn.functional.softmax(x_proj_reshaped, dim=1)\n\n        x_rproj_reshaped = x_proj_reshaped\n\n        x_n_state = torch.matmul(x_state_reshaped, x_proj_reshaped.permute(0, 2, 1))\n        if self.normalize:\n            x_n_state = x_n_state * (1. / x_state_reshaped.size(2))\n        x_n_rel = self.gcn(x_n_state)\n\n        x_state_reshaped = torch.matmul(x_n_rel, x_rproj_reshaped)\n        x_state = x_state_reshaped.view(n, self.num_s, *x.size()[2:])\n        out = x + (self.conv_extend(x_state))\n\n        return out\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\nclass PolypPVT(nn.Module):\n    def __init__(self, channel=32):\n        super(PolypPVT, self).__init__()\n\n        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n        path = '/kaggle/input/pretrained-pth/pvt_v2_b2.pth'\n        save_model = torch.load(path)\n        model_dict = self.backbone.state_dict()\n        state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n        model_dict.update(state_dict)\n        self.backbone.load_state_dict(model_dict)\n\n        self.Translayer2_0 = BasicConv2d(64, channel, 1)\n        self.Translayer2_1 = BasicConv2d(128, channel, 1)\n        self.Translayer3_1 = BasicConv2d(320, channel, 1)\n        self.Translayer4_1 = BasicConv2d(512, channel, 1)\n\n        self.CFM = CFM(channel)\n        self.ca = ChannelAttention(64)\n        self.sa = SpatialAttention()\n        self.SAM = SAM()\n        \n        self.down05 = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=True)\n        self.out_SAM = nn.Conv2d(channel, 1, 1)\n        self.out_CFM = nn.Conv2d(channel, 1, 1)\n\n\n    def forward(self, x):\n\n        # backbone\n        pvt = self.backbone(x)\n        x1 = pvt[0]\n        x2 = pvt[1]\n        x3 = pvt[2]\n        x4 = pvt[3]\n        \n        # CIM\n        x1 = self.ca(x1) * x1 # channel attention\n        cim_feature = self.sa(x1) * x1 # spatial attention\n\n\n        # CFM\n        x2_t = self.Translayer2_1(x2)  \n        x3_t = self.Translayer3_1(x3)  \n        x4_t = self.Translayer4_1(x4)  \n        cfm_feature = self.CFM(x4_t, x3_t, x2_t)\n\n        # SAM\n        T2 = self.Translayer2_0(cim_feature)\n        T2 = self.down05(T2)\n        sam_feature = self.SAM(cfm_feature, T2)\n\n        prediction1 = self.out_CFM(cfm_feature)\n        prediction2 = self.out_SAM(sam_feature)\n\n        prediction1_8 = F.interpolate(prediction1, scale_factor=8, mode='bilinear') \n        prediction2_8 = F.interpolate(prediction2, scale_factor=8, mode='bilinear')  \n        return prediction1_8, prediction2_8\n\n\nif __name__ == '__main__':\n    device = torch.device('cpu')  # Use CPU\n    \n    model = PolypPVT().to(device)\n    input_tensor = torch.randn(1, 3, 352, 352).to(device)\n\n    prediction1, prediction2 = model(input_tensor)\n    print(prediction1.size(), prediction2.size())","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:35:36.972483Z","iopub.execute_input":"2023-11-29T23:35:36.972840Z","iopub.status.idle":"2023-11-29T23:35:40.274874Z","shell.execute_reply.started":"2023-11-29T23:35:36.972806Z","shell.execute_reply":"2023-11-29T23:35:40.273923Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 352, 352]) torch.Size([1, 1, 352, 352])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"ResNet.py\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    # ResNet50 with two branches\n    def __init__(self):\n        # self.inplanes = 128\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(Bottleneck, 64, 3)\n        self.layer2 = self._make_layer(Bottleneck, 128, 4, stride=2)\n        self.layer3 = self._make_layer(Bottleneck, 256, 6, stride=2)\n        self.layer4 = self._make_layer(Bottleneck, 512, 3, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x1 = self.layer3_1(x)\n        x1 = self.layer4_1(x1)\n\n        x2 = self.layer3_2(x)\n        x2 = self.layer4_2(x2)\n\n        return x1, x2","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:35:40.276497Z","iopub.execute_input":"2023-11-29T23:35:40.276919Z","iopub.status.idle":"2023-11-29T23:35:40.303147Z","shell.execute_reply.started":"2023-11-29T23:35:40.276877Z","shell.execute_reply":"2023-11-29T23:35:40.302347Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"PraNet_ResNet.py\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport math\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass RFB(nn.Module):\n    # RFB-like multi-scale module\n    def __init__(self, in_channel, out_channel):\n        super(RFB, self).__init__()\n        self.relu = nn.ReLU(True)\n        self.branch0 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n        )\n        self.branch1 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(3, 1), padding=(1, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=3, dilation=3)\n        )\n        self.branch2 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 5), padding=(0, 2)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(5, 1), padding=(2, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=5, dilation=5)\n        )\n        self.branch3 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=7, dilation=7)\n        )\n        self.conv_cat = BasicConv2d(4*out_channel, out_channel, 3, padding=1)\n        self.conv_res = BasicConv2d(in_channel, out_channel, 1)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n\n        x = self.relu(x_cat + self.conv_res(x))\n        return x\n\n\nclass aggregation(nn.Module):\n    # dense aggregation, it can be replaced by other aggregation previous, such as DSS, amulet, and so on.\n    # used after MSF\n    def __init__(self, channel):\n        super(aggregation, self).__init__()\n        self.relu = nn.ReLU(True)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample5 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n\n        self.conv_concat2 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n        self.conv_concat3 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n        self.conv4 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n        self.conv5 = nn.Conv2d(3*channel, 1, 1)\n\n    def forward(self, x1, x2, x3):\n        x1_1 = x1\n        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n               * self.conv_upsample3(self.upsample(x2)) * x3\n\n        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n        x2_2 = self.conv_concat2(x2_2)\n\n        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n        x3_2 = self.conv_concat3(x3_2)\n\n        x = self.conv4(x3_2)\n        x = self.conv5(x)\n\n        return x\n\n\nclass CRANet(nn.Module):\n    # resnet based encoder decoder\n    def __init__(self, channel=32):\n        super(CRANet, self).__init__()\n\n        # ---- ResNet Backbone ----\n        self.resnet = ResNet()\n\n        # Receptive Field Block\n        self.rfb2_1 = RFB(512, channel)\n        self.rfb3_1 = RFB(1024, channel)\n        self.rfb4_1 = RFB(2048, channel)\n\n        # Partial Decoder\n        self.agg1 = aggregation(channel)\n        # self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n\n        # ---- reverse attention branch 4 ----\n        self.ra4_conv1 = BasicConv2d(2048, 256, kernel_size=1)\n        self.ra4_conv2 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv3 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv4 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv5 = BasicConv2d(256, 1, kernel_size=1)\n        # self.ra4_conv5_up = nn.ConvTranspose2d(1, 1, kernel_size=64, stride=32)\n\n        # ---- reverse attention branch 3 ----\n        # self.ra4_3 = nn.ConvTranspose2d(1, 1, kernel_size=4, stride=2)\n        self.ra3_conv1 = BasicConv2d(1024, 64, kernel_size=1)\n        self.ra3_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra3_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra3_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n        # self.ra3_conv4_up = nn.ConvTranspose2d(1, 1, kernel_size=32, stride=16)\n\n        # ---- reverse attention branch 2 ----\n        # self.ra3_2 = nn.ConvTranspose2d(1, 1, kernel_size=4, stride=2)\n        self.ra2_conv1 = BasicConv2d(512, 64, kernel_size=1)\n        self.ra2_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra2_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra2_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n        # self.ra2_conv4_up = nn.ConvTranspose2d(1, 1, kernel_size=16, stride=8)\n\n        # self.HA = HA()\n        if self.training:\n            self.initialize_weights()\n            # self.apply(CRANet.weights_init)\n\n    def forward(self, x):\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)      # bs, 64, 88, 88\n        x1 = self.resnet.layer1(x)      # bs, 256, 88, 88\n        x2 = self.resnet.layer2(x1)     # bs, 512, 44, 44\n\n        x3 = self.resnet.layer3(x2)     # bs, 1024, 22, 22\n        x4 = self.resnet.layer4(x3)     # bs, 2048, 11, 11\n        x2_rfb = self.rfb2_1(x2)        # channel -> 32\n        x3_rfb = self.rfb3_1(x3)        # channel -> 32\n        x4_rfb = self.rfb4_1(x4)        # channel -> 32\n\n        ra5_feat = self.agg1(x4_rfb, x3_rfb, x2_rfb)\n        lateral_map_5 = F.interpolate(ra5_feat, scale_factor=8, mode='bilinear')    # Sup-1 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_4 ----\n        crop_4 = F.interpolate(ra5_feat, scale_factor=0.25, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_4)) + 1\n        x = x.expand(-1, 2048, -1, -1).mul(x4)\n        x = self.ra4_conv1(x)\n        x = F.relu(self.ra4_conv2(x))\n        x = F.relu(self.ra4_conv3(x))\n        x = F.relu(self.ra4_conv4(x))\n        ra4_feat = self.ra4_conv5(x)\n        x = ra4_feat + crop_4\n        lateral_map_4 = F.interpolate(x, scale_factor=32, mode='bilinear')      # Sup-2 (bs, 1, 11, 11) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_3 ----\n        # x = F.interpolate(x, scale_factor=2, mode='bilinear')\n        crop_3 = F.interpolate(x, scale_factor=2, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_3)) + 1\n        x = x.expand(-1, 1024, -1, -1).mul(x3)\n        x = self.ra3_conv1(x)\n        x = F.relu(self.ra3_conv2(x))\n        x = F.relu(self.ra3_conv3(x))\n        ra3_feat = self.ra3_conv4(x)\n        x = ra3_feat + crop_3\n        lateral_map_3 = F.interpolate(x, scale_factor=16, mode='bilinear')\n        # lateral_map_3 = self.crop(self.ra3_conv4_up(x), x_size)  # NOTES: Sup-3 (bs, 1, 22, 22) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_2 ----\n        # x = self.ra3_2(x)\n        # crop_2 = self.crop(x, x2.size())\n        crop_2 = F.interpolate(x, scale_factor=2, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_2)) + 1\n        x = x.expand(-1, 512, -1, -1).mul(x2)\n        x = self.ra2_conv1(x)\n        x = F.relu(self.ra2_conv2(x))\n        x = F.relu(self.ra2_conv3(x))\n        ra2_feat = self.ra2_conv4(x)\n        x = ra2_feat + crop_2\n        lateral_map_2 = F.interpolate(x, scale_factor=8, mode='bilinear')\n        # lateral_map_2 = self.crop(self.ra2_conv4_up(x), x_size)  # NOTES: Sup-4 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n\n        return lateral_map_5, lateral_map_4, lateral_map_3, lateral_map_2\n\n    # def crop(self, upsampled, x_size):\n    #     c = (upsampled.size()[2] - x_size[2]) // 2\n    #     _c = x_size[2] - upsampled.size()[2] + c\n    #     assert(c >= 0)\n    #     if c == _c == 0:\n    #         return upsampled\n    #     return upsampled[:, :, c:_c, c:_c]\n\n    def initialize_weights(self):\n        res50 = models.resnet50(pretrained=True)\n        pretrained_dict = res50.state_dict()\n        all_params = {}\n        for k, v in self.resnet.state_dict().items():\n            if k in pretrained_dict.keys():\n                v = pretrained_dict[k]\n                all_params[k] = v\n        assert len(all_params.keys()) == len(self.resnet.state_dict().keys())\n        self.resnet.load_state_dict(all_params)\n\n    # @staticmethod\n    # def weights_init(m):\n    #     if isinstance(m, nn.Conv2d):\n    #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n    #         m.weight.data.normal_(0, math.sqrt(2. / n))\n    #     elif isinstance(m, nn.ConvTranspose2d):\n    #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n    #         m.weight.data.normal_(0, math.sqrt(2. / n))\n\nif __name__ == '__main__':\n    ras = CRANet()\n    input_tensor = torch.randn(1, 3, 352, 352)\n\n    out = ras(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:35:40.304385Z","iopub.execute_input":"2023-11-29T23:35:40.304675Z","iopub.status.idle":"2023-11-29T23:35:42.220282Z","shell.execute_reply.started":"2023-11-29T23:35:40.304650Z","shell.execute_reply":"2023-11-29T23:35:42.219299Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 221MB/s] \n","output_type":"stream"}]},{"cell_type":"markdown","source":"Res2Net_v1b.py","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport torch.nn.functional as F\n\n__all__ = ['Res2Net', 'res2net50_v1b', 'res2net101_v1b', 'res2net50_v1b_26w_4s']\n\nmodel_urls = {\n    'res2net50_v1b_26w_4s': '/kaggle/input/res2net50-v1b-26w-4s-3cf99910-pth/res2net50_v1b_26w_4s-3cf99910.pth',\n    'res2net101_v1b_26w_4s': '/kaggle/input/res2net101-v1b-26w-4s-0812c246-pth/res2net101_v1b_26w_4s-0812c246.pth',\n}\n\n\nclass Bottle2neck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale=4, stype='normal'):\n        \"\"\" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: 'normal': normal set. 'stage': first block of a new stage.\n        \"\"\"\n        super(Bottle2neck, self).__init__()\n\n        width = int(math.floor(planes * (baseWidth / 64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width * scale)\n\n        if scale == 1:\n            self.nums = 1\n        else:\n            self.nums = scale - 1\n        if stype == 'stage':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n            convs.append(nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False))\n            bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width = width\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n            if i == 0 or self.stype == 'stage':\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.convs[i](sp)\n            sp = self.relu(self.bns[i](sp))\n            if i == 0:\n                out = sp\n            else:\n                out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype == 'normal':\n            out = torch.cat((out, spx[self.nums]), 1)\n        elif self.scale != 1 and self.stype == 'stage':\n            out = torch.cat((out, self.pool(spx[self.nums])), 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Res2Net(nn.Module):\n\n    def __init__(self, block, layers, baseWidth=26, scale=4, num_classes=1000):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.AvgPool2d(kernel_size=stride, stride=stride,\n                             ceil_mode=True, count_include_pad=False),\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=1, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                            stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef res2net50_v1b(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b lib.\n    Res2Net-50 refers to the Res2Net-50_v1b_26w_4s.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n    return model\n\n\ndef res2net101_v1b(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n    return model\n\n\ndef res2net50_v1b_26w_4s(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model_state = torch.load('/kaggle/input/res2net50-v1b-26w-4s-3cf99910-pth/res2net50_v1b_26w_4s-3cf99910.pth')\n        model.load_state_dict(model_state)\n        # lib.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n    return model\n\n\ndef res2net101_v1b_26w_4s(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n    return model\n\n\ndef res2net152_v1b_26w_4s(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 8, 36, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net152_v1b_26w_4s']))\n    return model\n\n\n\n\n\nif __name__ == '__main__':\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    ras = CRANet().to(device)\n    input_tensor = torch.randn(1, 3, 352, 352).to(device)\n\n    out = ras(input_tensor)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:35:42.223483Z","iopub.execute_input":"2023-11-29T23:35:42.223994Z","iopub.status.idle":"2023-11-29T23:35:56.999581Z","shell.execute_reply.started":"2023-11-29T23:35:42.223961Z","shell.execute_reply":"2023-11-29T23:35:56.998797Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"PraNet_Res2Net.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass RFB_modified(nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super(RFB_modified, self).__init__()\n        self.relu = nn.ReLU(True)\n        self.branch0 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n        )\n        self.branch1 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(3, 1), padding=(1, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=3, dilation=3)\n        )\n        self.branch2 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 5), padding=(0, 2)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(5, 1), padding=(2, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=5, dilation=5)\n        )\n        self.branch3 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=7, dilation=7)\n        )\n        self.conv_cat = BasicConv2d(4*out_channel, out_channel, 3, padding=1)\n        self.conv_res = BasicConv2d(in_channel, out_channel, 1)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n\n        x = self.relu(x_cat + self.conv_res(x))\n        return x\n\n\nclass aggregation(nn.Module):\n    # dense aggregation, it can be replaced by other aggregation previous, such as DSS, amulet, and so on.\n    # used after MSF\n    def __init__(self, channel):\n        super(aggregation, self).__init__()\n        self.relu = nn.ReLU(True)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample5 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n\n        self.conv_concat2 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n        self.conv_concat3 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n        self.conv4 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n        self.conv5 = nn.Conv2d(3*channel, 1, 1)\n\n    def forward(self, x1, x2, x3):\n        x1_1 = x1\n        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n               * self.conv_upsample3(self.upsample(x2)) * x3\n\n        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n        x2_2 = self.conv_concat2(x2_2)\n\n        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n        x3_2 = self.conv_concat3(x3_2)\n\n        x = self.conv4(x3_2)\n        x = self.conv5(x)\n\n        return x\n\n\nclass PraNet(nn.Module):\n    # res2net based encoder decoder\n    def __init__(self, channel=32):\n        super(PraNet, self).__init__()\n        # ---- ResNet Backbone ----\n        self.resnet = res2net50_v1b_26w_4s(pretrained=True)\n        # ---- Receptive Field Block like module ----\n        self.rfb2_1 = RFB_modified(512, channel)\n        self.rfb3_1 = RFB_modified(1024, channel)\n        self.rfb4_1 = RFB_modified(2048, channel)\n        # ---- Partial Decoder ----\n        self.agg1 = aggregation(channel)\n        # ---- reverse attention branch 4 ----\n        self.ra4_conv1 = BasicConv2d(2048, 256, kernel_size=1)\n        self.ra4_conv2 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv3 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv4 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv5 = BasicConv2d(256, 1, kernel_size=1)\n        # ---- reverse attention branch 3 ----\n        self.ra3_conv1 = BasicConv2d(1024, 64, kernel_size=1)\n        self.ra3_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra3_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra3_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n        # ---- reverse attention branch 2 ----\n        self.ra2_conv1 = BasicConv2d(512, 64, kernel_size=1)\n        self.ra2_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra2_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra2_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)      # bs, 64, 88, 88\n        # ---- low-level features ----\n        x1 = self.resnet.layer1(x)      # bs, 256, 88, 88\n        x2 = self.resnet.layer2(x1)     # bs, 512, 44, 44\n\n        x3 = self.resnet.layer3(x2)     # bs, 1024, 22, 22\n        x4 = self.resnet.layer4(x3)     # bs, 2048, 11, 11\n        x2_rfb = self.rfb2_1(x2)        # channel -> 32\n        x3_rfb = self.rfb3_1(x3)        # channel -> 32\n        x4_rfb = self.rfb4_1(x4)        # channel -> 32\n\n        ra5_feat = self.agg1(x4_rfb, x3_rfb, x2_rfb)\n        lateral_map_5 = F.interpolate(ra5_feat, scale_factor=8, mode='bilinear')    # NOTES: Sup-1 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_4 ----\n        crop_4 = F.interpolate(ra5_feat, scale_factor=0.25, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_4)) + 1\n        x = x.expand(-1, 2048, -1, -1).mul(x4)\n        x = self.ra4_conv1(x)\n        x = F.relu(self.ra4_conv2(x))\n        x = F.relu(self.ra4_conv3(x))\n        x = F.relu(self.ra4_conv4(x))\n        ra4_feat = self.ra4_conv5(x)\n        x = ra4_feat + crop_4\n        lateral_map_4 = F.interpolate(x, scale_factor=32, mode='bilinear')  # NOTES: Sup-2 (bs, 1, 11, 11) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_3 ----\n        crop_3 = F.interpolate(x, scale_factor=2, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_3)) + 1\n        x = x.expand(-1, 1024, -1, -1).mul(x3)\n        x = self.ra3_conv1(x)\n        x = F.relu(self.ra3_conv2(x))\n        x = F.relu(self.ra3_conv3(x))\n        ra3_feat = self.ra3_conv4(x)\n        x = ra3_feat + crop_3\n        lateral_map_3 = F.interpolate(x, scale_factor=16, mode='bilinear')  # NOTES: Sup-3 (bs, 1, 22, 22) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_2 ----\n        crop_2 = F.interpolate(x, scale_factor=2, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_2)) + 1\n        x = x.expand(-1, 512, -1, -1).mul(x2)\n        x = self.ra2_conv1(x)\n        x = F.relu(self.ra2_conv2(x))\n        x = F.relu(self.ra2_conv3(x))\n        ra2_feat = self.ra2_conv4(x)\n        x = ra2_feat + crop_2\n        lateral_map_2 = F.interpolate(x, scale_factor=8, mode='bilinear')   # NOTES: Sup-4 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n\n        return lateral_map_5, lateral_map_4, lateral_map_3, lateral_map_2\n\n\nif __name__ == '__main__':\n    ras = PraNet().cuda()\n    input_tensor = torch.randn(1, 3, 352, 352).cuda()\n\n    out = ras(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:35:57.001069Z","iopub.execute_input":"2023-11-29T23:35:57.001367Z","iopub.status.idle":"2023-11-29T23:35:59.058484Z","shell.execute_reply.started":"2023-11-29T23:35:57.001340Z","shell.execute_reply":"2023-11-29T23:35:59.057652Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Utlis.py ","metadata":{}},{"cell_type":"markdown","source":"utils.py","metadata":{}},{"cell_type":"code","source":"!pip install thop","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:35:59.059822Z","iopub.execute_input":"2023-11-29T23:35:59.060105Z","iopub.status.idle":"2023-11-29T23:36:13.058830Z","shell.execute_reply.started":"2023-11-29T23:35:59.060079Z","shell.execute_reply":"2023-11-29T23:36:13.057740Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom thop import profile\nfrom thop import clever_format\n\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    For calibrating misalignment gradient via cliping gradient technique\n    :param optimizer:\n    :param grad_clip:\n    :return:\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef adjust_lr(optimizer, init_lr, epoch, decay_rate=0.1, decay_epoch=30):\n    decay = decay_rate ** (epoch // decay_epoch)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] *= decay\n\n\nclass AvgMeter(object):\n    def __init__(self, num=40):\n        self.num = num\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.losses = []\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        self.losses.append(val)\n\n    def show(self):\n        return torch.mean(torch.stack(self.losses[np.maximum(len(self.losses)-self.num, 0):]))\n\n\ndef CalParams(model, input_tensor):\n    \"\"\"\n    Usage:\n        Calculate Params and FLOPs via [THOP](https://github.com/Lyken17/pytorch-OpCounter)\n    Necessarity:\n        from thop import profile\n        from thop import clever_format\n    :param model:\n    :param input_tensor:\n    :return:\n    \"\"\"\n    flops, params = profile(model, inputs=(input_tensor,))\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print('[Statistics Information]\\nFLOPs: {}\\nParams: {}'.format(flops, params))","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:36:13.060561Z","iopub.execute_input":"2023-11-29T23:36:13.060951Z","iopub.status.idle":"2023-11-29T23:36:13.081797Z","shell.execute_reply.started":"2023-11-29T23:36:13.060911Z","shell.execute_reply":"2023-11-29T23:36:13.081100Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"format_conversion.py","metadata":{}},{"cell_type":"code","source":"!pip install libtiff","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:36:13.082932Z","iopub.execute_input":"2023-11-29T23:36:13.083204Z","iopub.status.idle":"2023-11-29T23:36:35.553206Z","shell.execute_reply.started":"2023-11-29T23:36:13.083180Z","shell.execute_reply":"2023-11-29T23:36:35.552037Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting libtiff\n  Downloading libtiff-0.4.2.tar.gz (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.0/130.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: libtiff\n  Building wheel for libtiff (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for libtiff: filename=libtiff-0.4.2-cp310-cp310-linux_x86_64.whl size=176064 sha256=a6db2cdf63359a1fdacd912ce7c3b7a413766c18d96566b2b70c82de845c9199\n  Stored in directory: /root/.cache/pip/wheels/65/5d/5d/90eac321b46b2ea36b0b3a180c60103bb05e01c2f1a93ceb5e\nSuccessfully built libtiff\nInstalling collected packages: libtiff\nSuccessfully installed libtiff-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport random\n\ndef tif2png(src_path, dst_path):\n    \"\"\"\n    Convert TIF/TIFF file to PNG file using PIL\n    \"\"\"\n    with Image.open(src_path) as img:\n        img.save(dst_path, \"PNG\")\n\ndef data_split(src_list):\n    \"\"\"\n    Randomly split a dataset\n    \"\"\"\n    counter_list = random.sample(range(0, len(src_list)), 200)\n    return counter_list\n\nif __name__ == '__main__':\n    src_dir = '/kaggle/input/traindatasetimages'\n    dst_dir = '/kaggle/input/traindatasetmasks'\n\n    os.makedirs(dst_dir, exist_ok=True)\n    \n    # Convert TIF to PNG\n    for img_name in os.listdir(src_dir):\n        if img_name.endswith('.tif') or img_name.endswith('.tiff'):\n            tif2png(os.path.join(src_dir, img_name),\n                    os.path.join(dst_dir, img_name.replace('.tif', '.png')))\n    \n    # Randomly split the dataset\n    file_list = os.listdir(dst_dir)\n    counter_list = data_split(file_list)\n    print(counter_list)  # Just an example, you might use this list for further operations\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:36:35.554691Z","iopub.execute_input":"2023-11-29T23:36:35.555001Z","iopub.status.idle":"2023-11-29T23:36:35.675167Z","shell.execute_reply.started":"2023-11-29T23:36:35.554970Z","shell.execute_reply":"2023-11-29T23:36:35.674343Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[60, 186, 70, 41, 142, 131, 12, 241, 27, 167, 226, 45, 150, 63, 33, 174, 47, 187, 232, 129, 112, 115, 138, 176, 122, 49, 195, 99, 205, 132, 245, 193, 199, 140, 34, 173, 134, 78, 68, 106, 98, 52, 224, 229, 15, 238, 28, 46, 234, 58, 38, 64, 156, 2, 7, 141, 171, 233, 162, 192, 244, 159, 169, 191, 196, 118, 127, 119, 76, 188, 113, 155, 74, 23, 124, 222, 136, 239, 36, 55, 149, 235, 1, 248, 85, 223, 214, 97, 202, 6, 101, 220, 81, 185, 110, 184, 83, 152, 133, 16, 13, 111, 50, 144, 67, 84, 8, 73, 231, 88, 20, 213, 107, 181, 80, 39, 177, 56, 66, 243, 225, 206, 190, 18, 104, 57, 161, 108, 180, 44, 31, 109, 194, 216, 208, 5, 61, 79, 121, 0, 217, 87, 246, 137, 14, 166, 242, 126, 117, 240, 22, 125, 145, 221, 24, 92, 100, 211, 135, 105, 17, 207, 139, 249, 148, 179, 35, 182, 120, 198, 247, 175, 210, 153, 160, 59, 151, 102, 10, 82, 53, 72, 37, 178, 71, 236, 26, 11, 93, 116, 90, 172, 48, 114, 230, 128, 4, 212, 19, 170]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"dataloader.py","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport numpy as np\nimport random\nimport torch\n\n\nclass PolypDataset(data.Dataset):\n    \"\"\"\n    dataloader for polyp segmentation tasks\n    \"\"\"\n    def __init__(self, image_root, gt_root, trainsize, augmentations):\n        self.trainsize = trainsize\n        self.augmentations = augmentations\n        print(self.augmentations)\n        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.png')]\n        self.images = sorted(self.images)\n        self.gts = sorted(self.gts)\n        self.filter_files()\n        self.size = len(self.images)\n        if self.augmentations == 'True':\n            print('Using RandomRotation, RandomFlip')\n            self.img_transform = transforms.Compose([\n                transforms.RandomRotation(90, resample=False, expand=False, center=None, fill=None),\n                transforms.RandomVerticalFlip(p=0.5),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.Resize((self.trainsize, self.trainsize)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406],\n                                     [0.229, 0.224, 0.225])])\n            self.gt_transform = transforms.Compose([\n                transforms.RandomRotation(90, resample=False, expand=False, center=None, fill=None),\n                transforms.RandomVerticalFlip(p=0.5),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.Resize((self.trainsize, self.trainsize)),\n                transforms.ToTensor()])\n            \n        else:\n            print('no augmentation')\n            self.img_transform = transforms.Compose([\n                transforms.Resize((self.trainsize, self.trainsize)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406],\n                                     [0.229, 0.224, 0.225])])\n            \n            self.gt_transform = transforms.Compose([\n                transforms.Resize((self.trainsize, self.trainsize)),\n                transforms.ToTensor()])\n            \n\n    def __getitem__(self, index):\n        \n        image = self.rgb_loader(self.images[index])\n        gt = self.binary_loader(self.gts[index])\n        \n        seed = np.random.randint(2147483647) # make a seed with numpy generator \n        random.seed(seed) # apply this seed to img tranfsorms\n        torch.manual_seed(seed) # needed for torchvision 0.7\n        if self.img_transform is not None:\n            image = self.img_transform(image)\n            \n        random.seed(seed) # apply this seed to img tranfsorms\n        torch.manual_seed(seed) # needed for torchvision 0.7\n        if self.gt_transform is not None:\n            gt = self.gt_transform(gt)\n        return image, gt\n\n    def filter_files(self):\n        assert len(self.images) == len(self.gts)\n        images = []\n        gts = []\n        for img_path, gt_path in zip(self.images, self.gts):\n            img = Image.open(img_path)\n            gt = Image.open(gt_path)\n            if img.size == gt.size:\n                images.append(img_path)\n                gts.append(gt_path)\n        self.images = images\n        self.gts = gts\n\n    def rgb_loader(self, path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')\n\n    def binary_loader(self, path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            # return img.convert('1')\n            return img.convert('L')\n\n    def resize(self, img, gt):\n        assert img.size == gt.size\n        w, h = img.size\n        if h < self.trainsize or w < self.trainsize:\n            h = max(h, self.trainsize)\n            w = max(w, self.trainsize)\n            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST)\n        else:\n            return img, gt\n\n    def __len__(self):\n        return self.size\n\n\ndef get_loader(image_root, gt_root, batchsize, trainsize, shuffle=True, num_workers=4, pin_memory=True, augmentation=False):\n\n    dataset = PolypDataset(image_root, gt_root, trainsize, augmentation)\n    data_loader = data.DataLoader(dataset=dataset,\n                                  batch_size=batchsize,\n                                  shuffle=shuffle,\n                                  num_workers=num_workers,\n                                  pin_memory=pin_memory)\n    return data_loader\n\n\nclass test_dataset:\n    def __init__(self, image_root, gt_root, testsize):\n        self.testsize = testsize\n        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.tif') or f.endswith('.png')]\n        self.images = sorted(self.images)\n        self.gts = sorted(self.gts)\n        self.transform = transforms.Compose([\n            transforms.Resize((self.testsize, self.testsize)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225])])\n        self.gt_transform = transforms.ToTensor()\n        self.size = len(self.images)\n        self.index = 0\n\n    def load_data(self):\n        image = self.rgb_loader(self.images[self.index])\n        image = self.transform(image).unsqueeze(0)\n        gt = self.binary_loader(self.gts[self.index])\n        name = self.images[self.index].split('/')[-1]\n        if name.endswith('.jpg'):\n            name = name.split('.jpg')[0] + '.png'\n        self.index += 1\n        return image, gt, name\n\n    def rgb_loader(self, path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')\n\n    def binary_loader(self, path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('L')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:36:35.676324Z","iopub.execute_input":"2023-11-29T23:36:35.676594Z","iopub.status.idle":"2023-11-29T23:36:35.707905Z","shell.execute_reply.started":"2023-11-29T23:36:35.676569Z","shell.execute_reply":"2023-11-29T23:36:35.706997Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Main ","metadata":{}},{"cell_type":"markdown","source":"Train.py","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport os\nimport argparse\nfrom datetime import datetime\n\n\nimport torch.nn.functional as F\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n\ndef structure_loss(pred, mask):\n    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n\n    pred = torch.sigmoid(pred)\n    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n    union = ((pred + mask) * weit).sum(dim=(2, 3))\n    wiou = 1 - (inter + 1) / (union - inter + 1)\n\n    return (wbce + wiou).mean()\n\n\ndef test(model, path, dataset):\n\n    data_path = os.path.join(path, dataset)\n    image_root = '{}/images/'.format(data_path)\n    gt_root = '{}/masks/'.format(data_path)\n    model.eval()\n    num1 = len(os.listdir(gt_root))\n    test_loader = test_dataset(image_root, gt_root, 352)\n    DSC = 0.0\n    for i in range(num1):\n        image, gt, name = test_loader.load_data()\n        gt = np.asarray(gt, np.float32)\n        gt /= (gt.max() + 1e-8)\n        image = image.to('cpu')\n\n        res, res1  = model(image)\n        # eval Dice\n        res = F.upsample(res + res1 , size=gt.shape, mode='bilinear', align_corners=False)\n        res = res.sigmoid().data.cpu().numpy().squeeze()\n        res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n        input = res\n        target = np.array(gt)\n        N = gt.shape\n        smooth = 1\n        input_flat = np.reshape(input, (-1))\n        target_flat = np.reshape(target, (-1))\n        intersection = (input_flat * target_flat)\n        dice = (2 * intersection.sum() + smooth) / (input.sum() + target.sum() + smooth)\n        dice = '{:.4f}'.format(dice)\n        dice = float(dice)\n        DSC = DSC + dice\n\n    return DSC / num1\n\n\n\ndef train(train_loader, model, optimizer, epoch, test_path):\n    model.train()\n    global best\n    size_rates = [0.75, 1, 1.25] \n    loss_P2_record = AvgMeter()\n    for i, pack in enumerate(train_loader, start=1):\n        for rate in size_rates:\n            optimizer.zero_grad()\n            # ---- data prepare ----\n            images, gts = pack\n            images = Variable(images)\n            gts = Variable(gts)\n            # ---- rescale ----\n            trainsize = int(round(opt.trainsize * rate / 32) * 32)\n            if rate != 1:\n                images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n                gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n            # ---- forward ----\n            P1, P2= model(images)\n            # ---- loss function ----\n            loss_P1 = structure_loss(P1, gts)\n            loss_P2 = structure_loss(P2, gts)\n            loss = loss_P1 + loss_P2 \n            # ---- backward ----\n            loss.backward()\n            clip_gradient(optimizer, opt.clip)\n            optimizer.step()\n            # ---- recording loss ----\n            if rate == 1:\n                loss_P2_record.update(loss_P2.data, opt.batchsize)\n        # ---- train visualization ----\n        if i % 20 == 0 or i == total_step:\n            print('{} Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], '\n                  ' lateral-5: {:0.4f}]'.\n                  format(datetime.now(), epoch, opt.epoch, i, total_step,\n                         loss_P2_record.show()))\n    # save model \n    save_path = (opt.train_save)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    torch.save(model.state_dict(), save_path +str(epoch)+ 'PolypPVT.pth')\n    # choose the best model\n\n    global dict_plot\n   \n    test1path = '/kaggle/input/testdataset/TestDataset'\n    if (epoch + 1) % 1 == 0:\n        for dataset in ['CVC-300', 'CVC-ClinicDB', 'Kvasir', 'CVC-ColonDB', 'ETIS-LaribPolypDB']:\n            dataset_dice = test(model, test1path, dataset)\n            logging.info('epoch: {}, dataset: {}, dice: {}'.format(epoch, dataset, dataset_dice))\n            print(dataset, ': ', dataset_dice)\n            dict_plot[dataset].append(dataset_dice)\n        meandice = test(model, test_path, 'test')\n        dict_plot['test'].append(meandice)\n        if meandice > best:\n            best = meandice\n            torch.save(model.state_dict(), save_path + 'PolypPVT.pth')\n            torch.save(model.state_dict(), save_path +str(epoch)+ 'PolypPVT-best.pth')\n            print('##############################################################################best', best)\n            logging.info('##############################################################################best:{}'.format(best))\n\n\ndef plot_train(dict_plot=None, name = None):\n    color = ['red', 'lawngreen', 'lime', 'gold', 'm', 'plum', 'blue']\n    line = ['-', \"--\"]\n    for i in range(len(name)):\n        plt.plot(dict_plot[name[i]], label=name[i], color=color[i], linestyle=line[(i + 1) % 2])\n        transfuse = {'CVC-300': 0.902, 'CVC-ClinicDB': 0.918, 'Kvasir': 0.918, 'CVC-ColonDB': 0.773,'ETIS-LaribPolypDB': 0.733, 'test':0.83}\n        plt.axhline(y=transfuse[name[i]], color=color[i], linestyle='-')\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"dice\")\n    plt.title('Train')\n    plt.legend()\n    plt.savefig('eval.png')\n    # plt.show()\n    \n    \nif __name__ == '__main__':\n    dict_plot = {'CVC-300':[], 'CVC-ClinicDB':[], 'Kvasir':[], 'CVC-ColonDB':[], 'ETIS-LaribPolypDB':[], 'test':[]}\n    name = ['CVC-300', 'CVC-ClinicDB', 'Kvasir', 'CVC-ColonDB', 'ETIS-LaribPolypDB', 'test']\n    ##################model_name#############################\n    model_name = 'PolypPVT'\n    ###############################################\n    opt = argparse.Namespace()\n    opt.epoch = 100\n    opt.lr = 1e-4\n    opt.optimizer = 'AdamW'\n    opt.augmentation = True\n    opt.batchsize = 16\n    opt.trainsize = 352\n    opt.clip = 0.5\n    opt.decay_rate = 0.1\n    opt.decay_epoch = 50\n    opt.train_path = '/kaggle/input/traindataset/TrainDataset'\n    opt.test_path = '/kaggle/input/testdataset/TestDataset'\n    opt.train_save = './model_pth/'+model_name+'/'\n\n    logging.basicConfig(filename='train_log.log',\n                        format='[%(asctime)s-%(filename)s-%(levelname)s:%(message)s]',\n                        level=logging.INFO, filemode='a', datefmt='%Y-%m-%d %I:%M:%S %p')\n\n\n    # ---- build models ----\n    # torch.cuda.set_device(0)  # set your gpu device\n   # ---- build models ----\n    model = PolypPVT()\n\n    best = 0\n\n    params = model.parameters()\n\n    if opt.optimizer == 'AdamW':\n        optimizer = torch.optim.AdamW(params, opt.lr, weight_decay=1e-4)\n    else:\n        optimizer = torch.optim.SGD(params, opt.lr, weight_decay=1e-4, momentum=0.9)\n\n\n    print(optimizer)\n    image_root = '{}/images/'.format(opt.train_path)\n    gt_root = '{}/masks/'.format(opt.train_path)\n\n    train_loader = get_loader(image_root, gt_root, batchsize=opt.batchsize, trainsize=opt.trainsize,\n                              augmentation=opt.augmentation)\n    total_step = len(train_loader)\n\n    print(\"#\" * 20, \"Start Training\", \"#\" * 20)\n\n    for epoch in range(1, opt.epoch):\n        adjust_lr(optimizer, opt.lr, epoch, 0.1, 200)\n        train(train_loader, model, optimizer, epoch, opt.test_path)\n    \n    # plot the eval.png in the training stage\n    # plot_train(dict_plot, name)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:36:35.709255Z","iopub.execute_input":"2023-11-29T23:36:35.709601Z","iopub.status.idle":"2023-11-29T23:55:35.664506Z","shell.execute_reply.started":"2023-11-29T23:36:35.709575Z","shell.execute_reply":"2023-11-29T23:55:35.663026Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0.0001\n)\nTrue\nno augmentation\n#################### Start Training ####################\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n  warnings.warn(warning.format(ret))\n","output_type":"stream"},{"name":"stdout","text":"2023-11-29 23:41:05.594239 Epoch [001/100], Step [0007/0007],  lateral-5: 1.2451]\nCVC-300 :  0.07424499999999998\nCVC-ClinicDB :  0.14480967741935488\nKvasir :  0.22192399999999998\nCVC-ColonDB :  0.1487959999999999\nETIS-LaribPolypDB :  0.08105816326530613\n##############################################################################best 0.14529999999999996\n2023-11-29 23:50:42.524571 Epoch [002/100], Step [0007/0007],  lateral-5: 1.1722]\nCVC-300 :  0.06282333333333331\nCVC-ClinicDB :  0.10817903225806451\nKvasir :  0.17635\nCVC-ColonDB :  0.09787999999999991\nETIS-LaribPolypDB :  0.06692193877551017\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 192\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, opt\u001b[38;5;241m.\u001b[39mepoch):\n\u001b[1;32m    191\u001b[0m     adjust_lr(optimizer, opt\u001b[38;5;241m.\u001b[39mlr, epoch, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# plot the eval.png in the training stage\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# plot_train(dict_plot, name)\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 115\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, epoch, test_path)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m, dataset_dice)\n\u001b[1;32m    114\u001b[0m     dict_plot[dataset]\u001b[38;5;241m.\u001b[39mappend(dataset_dice)\n\u001b[0;32m--> 115\u001b[0m meandice \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m dict_plot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(meandice)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meandice \u001b[38;5;241m>\u001b[39m best:\n","Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, path, dataset)\u001b[0m\n\u001b[1;32m     39\u001b[0m gt \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (gt\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m     40\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m res, res1  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# eval Dice\u001b[39;00m\n\u001b[1;32m     44\u001b[0m res \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mupsample(res \u001b[38;5;241m+\u001b[39m res1 , size\u001b[38;5;241m=\u001b[39mgt\u001b[38;5;241m.\u001b[39mshape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[2], line 187\u001b[0m, in \u001b[0;36mPolypPVT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# backbone\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     pvt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m pvt[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    189\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m pvt[\u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[1], line 356\u001b[0m, in \u001b[0;36mPyramidVisionTransformerImpr.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 356\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# x = self.head(x)\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","Cell \u001b[0;32mIn[1], line 322\u001b[0m, in \u001b[0;36mPyramidVisionTransformerImpr.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m x, H, W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed1(x)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1):\n\u001b[0;32m--> 322\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m    324\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[1], line 148\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, H, W)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, H, W):\n\u001b[0;32m--> 148\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    149\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x), H, W))\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[1], line 104\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, H, W)\u001b[0m\n\u001b[1;32m    101\u001b[0m k, v \u001b[38;5;241m=\u001b[39m kv[\u001b[38;5;241m0\u001b[39m], kv[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    103\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m--> 104\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n\u001b[1;32m    107\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, N, C)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Test.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport os, argparse\nfrom scipy import misc\nfrom lib.pvt import PolypPVT\nfrom utils.dataloader import test_dataset\nimport cv2\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--testsize', type=int, default=352, help='testing size')\n    parser.add_argument('--pth_path', type=str, default='./model_pth/PolypPVT.pth')\n    opt = parser.parse_args()\n    model = PolypPVT()\n    model.load_state_dict(torch.load(opt.pth_path))\n    model.cuda()\n    model.eval()\n    for _data_name in ['CVC-300', 'CVC-ClinicDB', 'Kvasir', 'CVC-ColonDB', 'ETIS-LaribPolypDB']:\n\n        ##### put data_path here #####\n        data_path = './dataset/TestDataset/{}'.format(_data_name)\n        ##### save_path #####\n        save_path = './result_map/PolypPVT/{}/'.format(_data_name)\n\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n        image_root = '{}/images/'.format(data_path)\n        gt_root = '{}/masks/'.format(data_path)\n        num1 = len(os.listdir(gt_root))\n        test_loader = test_dataset(image_root, gt_root, 352)\n        for i in range(num1):\n            image, gt, name = test_loader.load_data()\n            gt = np.asarray(gt, np.float32)\n            gt /= (gt.max() + 1e-8)\n            image = image.cuda()\n            P1,P2 = model(image)\n            res = F.upsample(P1+P2, size=gt.shape, mode='bilinear', align_corners=False)\n            res = res.sigmoid().data.cpu().numpy().squeeze()\n            res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n            cv2.imwrite(save_path+name, res*255)\n        print(_data_name, 'Finish!')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T23:55:35.665643Z","iopub.status.idle":"2023-11-29T23:55:35.665975Z","shell.execute_reply.started":"2023-11-29T23:55:35.665813Z","shell.execute_reply":"2023-11-29T23:55:35.665831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}